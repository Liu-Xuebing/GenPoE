#model_dir
model_name: "EleutherAI/gpt-j-6B"    # meta-llama/Llama-2-7b-hf, EleutherAI/gpt-j-6B
hypernetwork_ckpt: /data/liuxb/code/MMoE/{}_{}_hypernetwork.pth

#train_dir
NQ_train_dataset: /data/liuxb/datasets/NQ/NQ_train_results.json
TQA_train_dataset: /data/liuxb/datasets/TQA/TQA_train_results.json
SQuAD_train_dataset: /data/liuxb/datasets/SQuAD/train-v2.0.json

#test_dir
data_name: NQ
NQ_test_dataset_file: /data/liuxb/datasets/NQ/NQ_test_rerank_results.json  #/data3/liuxb/datasets/NQ/NQ_test_rerank_results.json
TQA_test_dataset_file: /data/liuxb/datasets/TQA/TQA_test_rerank_results.json  #/data3/liuxb/datasets/NQ/NQ_test_rerank_results.json
SQuAD_test_dataset_file: /data/liuxb/datasets/SQuAD/dev-v2.0.json  #/data3/liuxb/datasets/NQ/NQ_test_rerank_results.json

#model
half: False
train_batch_size: 1
valid_batch_size: 1

#train
learning_rate: 1e-4
learning_rate_min: 1e-6
single_layer: 11
num_experts: 1